---
title: "Thoughts on Sarcasm and AI"
startDate: 2025-09-06
description: "Recap AI Dev Europe: Unlocking Multimodal Sarcasm Analysis With NLP and Facial Expressions"
image:
   url: "/images/posts/2025-09-06.png"
   alt: "ML and AI Architecture"
tags: ["Multimodal Classification", "Unimodeal Classification", "AI Dev Europe", "David Von Thenen", "Sarcasm Detection"]
canonical: "https://github.com/davidvonthenen/2025-ai-dev-europe#"
---
The best way to learn is to reword, recap, summarize and then teach it to someone else.
I loved this presentation by David at AI Dev Europe.  It is a natural extension (next step) to what I had in mind
for my chatbot.

## Introduction
Sarcasm is a difficult problem in natural language processing (NLP) because it often relies on context, tone, and
non-verbal cues that are not present in text alone.  The very definition of sarcasm, if you think about it, is
rooted in context tied to culture and, a specific time in history.  How many times have you tried sarcasm in a group
conversation and fell flat on your face?  Or worse, offended someone?  Does anyone risk sarcasm in a high stakes
business environment?

Now consider teaching an inanimate, non-human entity to understand sarcasm.
It is not a pattern in word play, it is the words, combined with facial expressions and tone of voice.  Some people
communicate sarcasm with a straight face and monotone voice, others are more obvious.  We would need to train based
on context.

## Main Content
In this context, there are three main parts to this problem: dataset, capturing visual cues, audio cues.  Arguably,
the defining parameters of the problem, along with the dataset are the most difficult to determine.  He decides to
test with clips from "The Big Bang Theory", "Friends", "The Golden Girls" - Speaker dependent/independent
classification for training, validation and test.

taking a break.  more to come...

## Key Takeaways
- is it worth it?
- the modals (what I call in my head as faucets or slices or perspectives or views of a model) is how you break a
big problem into smaller problems.

## Conclusion
I suppose one can infer that in human generated literature, the sequence in which words are used is also a point of
contention.  I recently read a book on writing, "Eats, shoots & leaves" by Lynne Truss, which is a humorous take on
grammar and punctuation.  The author makes the point that the placement of a comma can change the meaning of a sentence.
What about different languages?  Chinese?  Chinese proverbs?!?   Will the other languages who do not participate in
this AI revolution die out simply because they have reduced efficiency in usage, which is the whole point of a
language (to be used to communicate)?

The reason the above is not production quality is the reason that the dataset is small, it does not quite consider
all cases.

A year ago, when I looked into developing a storytelling LLM to generate a compelling storyline as part of a cache
of assets for video games, I had considered a similar problem.  The 12 aspects that researchers have looked at that
makes a compelling story are: plot development, storytelling, character development, writing style, story planning,
common tropes, world-building, pacing, dialogue, emotional impact, originality, and genre conventions. When you
think about it, it all pertains to the human condition, how we live our lives, what is contextually important in our
time, etc.  Even something as basic as how your skin crawls when placed in a particular situation, what are you
triggers, the feeling and the glorious chemical reactions of a feeling and how every unique physical body percieves
it, is different.

Are we to teach an LLM to understand the human condition?  Or are we to teach it to understand us, by endlessly
refining modal classifications?  I suppose we could, we have done it before, the millionth or billionth refinement
in search of the truth.  In a biological sense, the price is high, it may be unsustainable.

On the other side, I love what has come out of AI.  I have found my love for coding all over again, I love creating
and thinking through these problems.

